{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import PIDSimulation as pidsim\n",
    "try:\n",
    "    import seaborn as sns\n",
    "except:\n",
    "    !pip install seaborn\n",
    "    import seaborn as sns\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "except:\n",
    "    !pip install tensorflow\n",
    "    import tensorflow as ts\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "try:\n",
    "    from deepevolution import wrap_keras\n",
    "except:\n",
    "    !pip install deepevolution\n",
    "    from deepevolution import wrap_keras\n",
    "    \n",
    "wrap_keras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Tp</th>\n",
       "      <th>Proportional</th>\n",
       "      <th>Integral</th>\n",
       "      <th>Derivative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137613</th>\n",
       "      <td>17.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119939</th>\n",
       "      <td>17.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77665</th>\n",
       "      <td>15.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72519</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221327</th>\n",
       "      <td>20.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197778</th>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15299</th>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166731</th>\n",
       "      <td>18.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55301</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125050</th>\n",
       "      <td>17.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191786 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Area  Beta   Tp  Proportional  Integral  Derivative\n",
       "137613  17.0   9.5  0.9           4.0       0.3         2.4\n",
       "119939  17.0   5.0  0.4          16.0       1.3         0.4\n",
       "77665   15.0   5.5  0.5          14.0       1.5         0.4\n",
       "72519   15.0   2.0  1.0           6.0       0.5         1.2\n",
       "221327  20.0   7.0  0.4           9.0       0.1         1.2\n",
       "...      ...   ...  ...           ...       ...         ...\n",
       "197778  19.0   9.0  0.9          10.0       0.5         0.8\n",
       "15299    9.0   9.0  0.5           7.0       0.7         0.4\n",
       "166731  18.0   9.5  0.7          13.0       0.7         0.4\n",
       "55301   14.0   1.5  0.8          15.0       1.5         0.4\n",
       "125050  17.0   7.0  0.1          15.0       0.9         0.4\n",
       "\n",
       "[191786 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the data\n",
    "dataset = pd.read_csv(\"sortedDataTop1.csv\")\n",
    "dataset_copy = dataset.copy()\n",
    "# split it into 2 pieces - train and test, test being 80% of all\n",
    "train_dataset = dataset_copy.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset_copy.drop(train_dataset.index)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Proportional</th>\n",
       "      <th>Integral</th>\n",
       "      <th>Derivative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137613</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119939</th>\n",
       "      <td>16.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77665</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72519</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221327</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197778</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15299</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166731</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55301</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125050</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191786 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Proportional  Integral  Derivative\n",
       "137613           4.0       0.3         2.4\n",
       "119939          16.0       1.3         0.4\n",
       "77665           14.0       1.5         0.4\n",
       "72519            6.0       0.5         1.2\n",
       "221327           9.0       0.1         1.2\n",
       "...              ...       ...         ...\n",
       "197778          10.0       0.5         0.8\n",
       "15299            7.0       0.7         0.4\n",
       "166731          13.0       0.7         0.4\n",
       "55301           15.0       1.5         0.4\n",
       "125050          15.0       0.9         0.4\n",
       "\n",
       "[191786 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not sure the reason for the code below, it doesn't work as intended\n",
    "# split features from labels\n",
    "# warning: run below code only once\n",
    "# it throws KeyError if argument is already popped\n",
    "\n",
    "if True:\n",
    "    train_labels_prop = train_dataset.pop(\"Proportional\")\n",
    "    train_labels_int = train_dataset.pop(\"Integral\")\n",
    "    train_labels_der = train_dataset.pop(\"Derivative\")\n",
    "    data_train = {\n",
    "        'Proportional': train_labels_prop,\n",
    "        'Integral': train_labels_int,\n",
    "        'Derivative': train_labels_der\n",
    "    }\n",
    "    train_labels = pd.DataFrame(data_train)\n",
    "    # same for test\n",
    "    test_labels_prop = test_dataset.pop(\"Proportional\")\n",
    "    test_labels_int = test_dataset.pop(\"Integral\")\n",
    "    test_labels_der = test_dataset.pop(\"Derivative\")\n",
    "    data_test = {\n",
    "        'Proportional': test_labels_prop,\n",
    "        'Integral': test_labels_int,\n",
    "        'Derivative': test_labels_der\n",
    "    }\n",
    "    test_labels = pd.DataFrame(data_test)\n",
    "\n",
    "train_labels\n",
    "# test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Area</th>\n",
       "      <td>191786.0</td>\n",
       "      <td>15.926381</td>\n",
       "      <td>3.693254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beta</th>\n",
       "      <td>191786.0</td>\n",
       "      <td>6.080413</td>\n",
       "      <td>2.660454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>8.5</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tp</th>\n",
       "      <td>191786.0</td>\n",
       "      <td>0.549178</td>\n",
       "      <td>0.274814</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count       mean       std  min   25%   50%   75%   max\n",
       "Area  191786.0  15.926381  3.693254  1.0  14.0  17.0  19.0  20.0\n",
       "Beta  191786.0   6.080413  2.660454  0.0   4.0   6.5   8.5   9.5\n",
       "Tp    191786.0   0.549178  0.274814  0.1   0.3   0.5   0.8   1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some statistics about the values\n",
    "# sns.pairplot(dataset)\n",
    "train_stats = train_dataset.describe()\n",
    "train_stats = train_stats.transpose()\n",
    "# train_stats\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Area    15.926381\n",
       "Beta     6.080413\n",
       "Tp       0.549178\n",
       "Name: mean, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the stats from above for denormalization in later use\n",
    "stats_dir = 'D:\\\\Dev\\\\PyCharmProjects\\\\ISSlab1\\\\programs\\\\'\n",
    "file_stats = open(str(stats_dir) + \"GAPID_stats.txt\", \"w\")\n",
    "# save the mean\n",
    "# file_stats.write(\"train_stats['mean'])\\n\")\n",
    "sentence = \"\".join(str(train_stats['mean']) + \"\\n\")\n",
    "file_stats.write(sentence)\n",
    "# save the sigma\n",
    "# file_stats.write(\"train_stats['std'])\\n\")\n",
    "sentence = \"\".join(str(train_stats['std']) + \"\\n\")\n",
    "file_stats.write(sentence)\n",
    "# finish\n",
    "file_stats.close()\n",
    "train_stats['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137613</th>\n",
       "      <td>0.290697</td>\n",
       "      <td>1.285340</td>\n",
       "      <td>1.276579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119939</th>\n",
       "      <td>0.290697</td>\n",
       "      <td>-0.406101</td>\n",
       "      <td>-0.542831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77665</th>\n",
       "      <td>-0.250831</td>\n",
       "      <td>-0.218163</td>\n",
       "      <td>-0.178949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72519</th>\n",
       "      <td>-0.250831</td>\n",
       "      <td>-1.533728</td>\n",
       "      <td>1.640461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221327</th>\n",
       "      <td>1.102989</td>\n",
       "      <td>0.345651</td>\n",
       "      <td>-0.542831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197778</th>\n",
       "      <td>0.832225</td>\n",
       "      <td>1.097402</td>\n",
       "      <td>1.276579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15299</th>\n",
       "      <td>-1.875414</td>\n",
       "      <td>1.097402</td>\n",
       "      <td>-0.178949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166731</th>\n",
       "      <td>0.561461</td>\n",
       "      <td>1.285340</td>\n",
       "      <td>0.548815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55301</th>\n",
       "      <td>-0.521595</td>\n",
       "      <td>-1.721666</td>\n",
       "      <td>0.912697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125050</th>\n",
       "      <td>0.290697</td>\n",
       "      <td>0.345651</td>\n",
       "      <td>-1.634476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191786 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Area      Beta        Tp\n",
       "137613  0.290697  1.285340  1.276579\n",
       "119939  0.290697 -0.406101 -0.542831\n",
       "77665  -0.250831 -0.218163 -0.178949\n",
       "72519  -0.250831 -1.533728  1.640461\n",
       "221327  1.102989  0.345651 -0.542831\n",
       "...          ...       ...       ...\n",
       "197778  0.832225  1.097402  1.276579\n",
       "15299  -1.875414  1.097402 -0.178949\n",
       "166731  0.561461  1.285340  0.548815\n",
       "55301  -0.521595 -1.721666  0.912697\n",
       "125050  0.290697  0.345651 -1.634476\n",
       "\n",
       "[191786 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize the data\n",
    "# here we have 3 columns in x but 6 columns in train stats so we get 3 NaNs\n",
    "# changed command order to fix it, but previously it was [statistics block] -> [pop labels block]\n",
    "def norm(x):\n",
    "    return (x - train_stats['mean']) / train_stats['std']\n",
    "normed_train_dataset = norm(train_dataset)\n",
    "normed_test_dataset = norm(test_dataset)\n",
    "# normed_train_dataset = train_dataset.copy()\n",
    "# normed_test_dataset = test_dataset.copy()\n",
    "normed_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 3)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                48        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 39        \n",
      "=================================================================\n",
      "Total params: 99\n",
      "Trainable params: 99\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "inp = keras.Input(3)\n",
    "layer = layers.Dense(3)(inp)\n",
    "layer = layers.Dense(12)(layer)\n",
    "layer = layers.Dense(3)(layer)\n",
    "model = keras.Model(inp, layer)\n",
    "model.compile(loss='mse', metrics=['mae', 'mse'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Area     Beta        Tp\n",
      "137613  0.290697  1.28534  1.276579\n",
      "        Proportional  Integral  Derivative\n",
      "137613           4.0       0.3         2.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.0201031 , -0.22196774, -0.5050521 ],\n",
       "       [-0.29886323,  0.05928773, -0.08846264],\n",
       "       [ 0.13260081,  0.04601061,  0.2173932 ],\n",
       "       [ 1.5182934 ,  0.02728176,  0.44842887],\n",
       "       [-1.0370466 , -0.05111544, -0.7855576 ],\n",
       "       [-1.2398639 , -0.07659042, -0.6206879 ],\n",
       "       [-0.34469062, -0.1922827 , -0.7704314 ],\n",
       "       [-0.41897628, -0.11485807, -0.77646136],\n",
       "       [ 0.6012964 ,  0.11453079,  1.0388815 ],\n",
       "       [-0.78938246, -0.03767563, -0.46924177]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if everythings connected and no errors are given \n",
    "example_batch = normed_train_dataset[:10]\n",
    "single_example = normed_train_dataset[:1]\n",
    "single_label = train_labels[:1]\n",
    "print(single_example)\n",
    "print(single_label)\n",
    "example_result = model.predict(example_batch)\n",
    "example_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122.19284187131561\n"
     ]
    }
   ],
   "source": [
    "# fitness function\n",
    "def fitness_func(model, X, Y):\n",
    "    value = model.evaluate(X, Y, batch_size=2048, verbose=0)[0]\n",
    "    value = 1 / value * 1000\n",
    "    return value\n",
    "print(fitness_func(model, single_example, single_label))\n",
    "# retruns [loss_value(mse) and metrics] so for ours it's:\n",
    "# retrurn [mse, mae, mse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Generation 1 / 500] score: 14.9823 (best: 15.0084; std: 0.0251)\n",
      "[Generation 2 / 500] score: 15.0707 (best: 15.1309; std: 0.0522)\n",
      "[Generation 3 / 500] score: 15.1615 (best: 15.283; std: 0.1094)\n",
      "[Generation 4 / 500] score: 15.2806 (best: 15.2908; std: 0.0116)\n",
      "[Generation 5 / 500] score: 15.3426 (best: 15.3944; std: 0.0518)\n",
      "[Generation 6 / 500] score: 15.3919 (best: 15.402; std: 0.0116)\n",
      "[Generation 7 / 500] score: 15.4318 (best: 15.4418; std: 0.0087)\n",
      "[Generation 8 / 500] score: 15.4698 (best: 15.4992; std: 0.0287)\n",
      "[Generation 9 / 500] score: 15.5274 (best: 15.6063; std: 0.0692)\n",
      "[Generation 10 / 500] score: 15.6109 (best: 15.6225; std: 0.0101)\n",
      "[Generation 11 / 500] score: 15.6452 (best: 15.6539; std: 0.0075)\n",
      "[Generation 12 / 500] score: 15.6916 (best: 15.7156; std: 0.023)\n",
      "[Generation 13 / 500] score: 15.7376 (best: 15.7757; std: 0.0332)\n",
      "[Generation 14 / 500] score: 15.7872 (best: 15.816; std: 0.0252)\n",
      "[Generation 15 / 500] score: 15.8202 (best: 15.8277; std: 0.0065)\n",
      "[Generation 16 / 500] score: 15.8788 (best: 15.8986; std: 0.0181)\n",
      "[Generation 17 / 500] score: 15.9676 (best: 15.9973; std: 0.0258)\n",
      "[Generation 18 / 500] score: 16.0777 (best: 16.133; std: 0.0484)\n",
      "[Generation 19 / 500] score: 16.149 (best: 16.1788; std: 0.0259)\n",
      "[Generation 20 / 500] score: 16.1649 (best: 16.1788; std: 0.0122)\n",
      "[Generation 21 / 500] score: 16.2048 (best: 16.2342; std: 0.0279)\n",
      "[Generation 22 / 500] score: 16.2671 (best: 16.3024; std: 0.0341)\n",
      "[Generation 23 / 500] score: 16.3038 (best: 16.3105; std: 0.0061)\n",
      "[Generation 24 / 500] score: 16.3198 (best: 16.3378; std: 0.0156)\n",
      "[Generation 25 / 500] score: 16.3817 (best: 16.4173; std: 0.0308)\n",
      "[Generation 26 / 500] score: 16.4423 (best: 16.4905; std: 0.0418)\n",
      "[Generation 27 / 500] score: 16.4922 (best: 16.5076; std: 0.0146)\n",
      "[Generation 28 / 500] score: 16.5109 (best: 16.515; std: 0.0038)\n",
      "[Generation 29 / 500] score: 16.582 (best: 16.6343; std: 0.0469)\n",
      "[Generation 30 / 500] score: 16.6109 (best: 16.6343; std: 0.0372)\n",
      "[Generation 31 / 500] score: 16.6822 (best: 16.7703; std: 0.0763)\n",
      "[Generation 32 / 500] score: 16.7682 (best: 16.8548; std: 0.0876)\n",
      "[Generation 33 / 500] score: 16.8709 (best: 16.8784; std: 0.0067)\n",
      "[Generation 34 / 500] score: 16.9204 (best: 16.9499; std: 0.0367)\n",
      "[Generation 35 / 500] score: 16.9618 (best: 17.0037; std: 0.0373)\n",
      "[Generation 36 / 500] score: 17.0473 (best: 17.0806; std: 0.0395)\n",
      "[Generation 37 / 500] score: 17.0581 (best: 17.0806; std: 0.0222)\n",
      "[Generation 38 / 500] score: 17.0932 (best: 17.1268; std: 0.0294)\n",
      "[Generation 39 / 500] score: 17.1419 (best: 17.1697; std: 0.0241)\n",
      "[Generation 40 / 500] score: 17.1837 (best: 17.207; std: 0.0203)\n",
      "[Generation 41 / 500] score: 17.2042 (best: 17.207; std: 0.0037)\n",
      "[Generation 42 / 500] score: 17.2094 (best: 17.2155; std: 0.0053)\n",
      "[Generation 43 / 500] score: 17.2425 (best: 17.2685; std: 0.0257)\n",
      "[Generation 44 / 500] score: 17.268 (best: 17.2773; std: 0.0096)\n",
      "[Generation 45 / 500] score: 17.3294 (best: 17.3539; std: 0.0265)\n",
      "[Generation 46 / 500] score: 17.3869 (best: 17.4179; std: 0.0277)\n",
      "[Generation 47 / 500] score: 17.433 (best: 17.4758; std: 0.0376)\n",
      "[Generation 48 / 500] score: 17.4721 (best: 17.4937; std: 0.0237)\n",
      "[Generation 49 / 500] score: 17.6031 (best: 17.719; std: 0.1015)\n",
      "[Generation 50 / 500] score: 17.7568 (best: 17.8083; std: 0.0451)\n",
      "[Generation 51 / 500] score: 17.8836 (best: 17.9768; std: 0.0857)\n",
      "[Generation 52 / 500] score: 17.9701 (best: 17.9768; std: 0.0106)\n",
      "[Generation 53 / 500] score: 17.995 (best: 18.0277; std: 0.0284)\n",
      "[Generation 54 / 500] score: 18.063 (best: 18.104; std: 0.0384)\n",
      "[Generation 55 / 500] score: 18.126 (best: 18.1478; std: 0.0219)\n",
      "[Generation 56 / 500] score: 18.1572 (best: 18.1858; std: 0.0252)\n",
      "[Generation 57 / 500] score: 18.2335 (best: 18.2443; std: 0.0093)\n",
      "[Generation 58 / 500] score: 18.2812 (best: 18.3106; std: 0.0296)\n",
      "[Generation 59 / 500] score: 18.3903 (best: 18.4744; std: 0.0768)\n",
      "[Generation 60 / 500] score: 18.4454 (best: 18.4744; std: 0.0406)\n",
      "[Generation 61 / 500] score: 18.5133 (best: 18.5382; std: 0.034)\n",
      "[Generation 62 / 500] score: 18.5754 (best: 18.6413; std: 0.0572)\n",
      "[Generation 63 / 500] score: 18.5987 (best: 18.6413; std: 0.048)\n",
      "[Generation 64 / 500] score: 18.6714 (best: 18.7; std: 0.0293)\n",
      "[Generation 65 / 500] score: 18.7504 (best: 18.7716; std: 0.0328)\n",
      "[Generation 66 / 500] score: 18.9018 (best: 19.0034; std: 0.1181)\n",
      "[Generation 67 / 500] score: 18.9983 (best: 19.0509; std: 0.0554)\n",
      "[Generation 68 / 500] score: 19.1472 (best: 19.2144; std: 0.0855)\n",
      "[Generation 69 / 500] score: 19.2376 (best: 19.2688; std: 0.0281)\n",
      "[Generation 70 / 500] score: 19.3345 (best: 19.4044; std: 0.0679)\n",
      "[Generation 71 / 500] score: 19.4208 (best: 19.4962; std: 0.0687)\n",
      "[Generation 72 / 500] score: 19.5105 (best: 19.5265; std: 0.0152)\n",
      "[Generation 73 / 500] score: 19.5643 (best: 19.598; std: 0.036)\n",
      "[Generation 74 / 500] score: 19.6287 (best: 19.6434; std: 0.024)\n",
      "[Generation 75 / 500] score: 19.8225 (best: 19.9716; std: 0.1526)\n",
      "[Generation 76 / 500] score: 19.9799 (best: 20.0363; std: 0.0528)\n",
      "[Generation 77 / 500] score: 20.233 (best: 20.3216; std: 0.1127)\n",
      "[Generation 78 / 500] score: 20.3135 (best: 20.3479; std: 0.039)\n",
      "[Generation 79 / 500] score: 20.3533 (best: 20.3654; std: 0.0105)\n",
      "[Generation 80 / 500] score: 20.5254 (best: 20.6917; std: 0.1632)\n",
      "[Generation 81 / 500] score: 20.6936 (best: 20.7411; std: 0.0465)\n",
      "[Generation 82 / 500] score: 20.7954 (best: 20.9099; std: 0.0992)\n",
      "[Generation 83 / 500] score: 20.9062 (best: 20.918; std: 0.014)\n",
      "[Generation 84 / 500] score: 20.982 (best: 20.989; std: 0.0099)\n",
      "[Generation 85 / 500] score: 21.0673 (best: 21.1301; std: 0.0701)\n",
      "[Generation 86 / 500] score: 21.2034 (best: 21.27; std: 0.0615)\n",
      "[Generation 87 / 500] score: 21.3425 (best: 21.4614; std: 0.1038)\n",
      "[Generation 88 / 500] score: 21.601 (best: 21.9159; std: 0.2733)\n",
      "[Generation 89 / 500] score: 21.845 (best: 21.9159; std: 0.0633)\n",
      "[Generation 90 / 500] score: 21.9942 (best: 22.0235; std: 0.0254)\n",
      "[Generation 91 / 500] score: 22.1833 (best: 22.2535; std: 0.0637)\n",
      "[Generation 92 / 500] score: 22.4061 (best: 22.5279; std: 0.13)\n",
      "[Generation 93 / 500] score: 22.574 (best: 22.7186; std: 0.128)\n",
      "[Generation 94 / 500] score: 22.7037 (best: 22.7186; std: 0.0176)\n",
      "[Generation 95 / 500] score: 22.8262 (best: 22.8904; std: 0.0788)\n",
      "[Generation 96 / 500] score: 22.9209 (best: 22.9465; std: 0.0283)\n",
      "[Generation 97 / 500] score: 23.0895 (best: 23.3963; std: 0.2659)\n",
      "[Generation 98 / 500] score: 23.3385 (best: 23.5709; std: 0.266)\n",
      "[Generation 99 / 500] score: 23.5758 (best: 23.6063; std: 0.0284)\n",
      "[Generation 100 / 500] score: 23.6707 (best: 23.756; std: 0.077)\n",
      "[Generation 101 / 500] score: 23.7798 (best: 23.8344; std: 0.0474)\n",
      "[Generation 102 / 500] score: 23.7805 (best: 23.8344; std: 0.0467)\n",
      "[Generation 103 / 500] score: 23.8751 (best: 23.8947; std: 0.0171)\n",
      "[Generation 104 / 500] score: 23.9916 (best: 24.06; std: 0.0863)\n",
      "[Generation 105 / 500] score: 24.2698 (best: 24.3832; std: 0.1015)\n",
      "[Generation 106 / 500] score: 24.5352 (best: 24.6733; std: 0.1455)\n",
      "[Generation 107 / 500] score: 24.6715 (best: 24.6955; std: 0.0249)\n",
      "[Generation 108 / 500] score: 24.706 (best: 24.7491; std: 0.039)\n",
      "[Generation 109 / 500] score: 24.7935 (best: 24.9328; std: 0.1232)\n",
      "[Generation 110 / 500] score: 25.0236 (best: 25.1924; std: 0.1463)\n",
      "[Generation 111 / 500] score: 25.4102 (best: 25.5424; std: 0.1901)\n",
      "[Generation 112 / 500] score: 25.543 (best: 25.5908; std: 0.0475)\n",
      "[Generation 113 / 500] score: 25.6365 (best: 25.7564; std: 0.1048)\n",
      "[Generation 114 / 500] score: 25.7907 (best: 25.8865; std: 0.0841)\n",
      "[Generation 115 / 500] score: 26.0449 (best: 26.0819; std: 0.0367)\n",
      "[Generation 116 / 500] score: 26.1856 (best: 26.3074; std: 0.1091)\n",
      "[Generation 117 / 500] score: 26.5607 (best: 26.781; std: 0.2385)\n",
      "[Generation 118 / 500] score: 26.7365 (best: 26.781; std: 0.0548)\n",
      "[Generation 119 / 500] score: 26.9867 (best: 27.1148; std: 0.162)\n",
      "[Generation 120 / 500] score: 27.2304 (best: 27.3884; std: 0.1416)\n",
      "[Generation 121 / 500] score: 27.466 (best: 27.7554; std: 0.2595)\n",
      "[Generation 122 / 500] score: 27.7418 (best: 27.7572; std: 0.0251)\n",
      "[Generation 123 / 500] score: 28.1439 (best: 28.2583; std: 0.1682)\n",
      "[Generation 124 / 500] score: 28.3894 (best: 28.6872; std: 0.2585)\n",
      "[Generation 125 / 500] score: 28.7085 (best: 28.8987; std: 0.1805)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Generation 126 / 500] score: 28.985 (best: 29.1046; std: 0.1069)\n",
      "[Generation 127 / 500] score: 29.2069 (best: 29.2745; std: 0.0674)\n",
      "[Generation 128 / 500] score: 29.3698 (best: 29.4348; std: 0.0843)\n",
      "[Generation 129 / 500] score: 29.5003 (best: 29.5167; std: 0.0156)\n",
      "[Generation 130 / 500] score: 29.8373 (best: 30.0891; std: 0.2385)\n",
      "[Generation 131 / 500] score: 30.0756 (best: 30.0891; std: 0.0233)\n",
      "[Generation 132 / 500] score: 30.1605 (best: 30.2084; std: 0.0484)\n",
      "[Generation 133 / 500] score: 30.4706 (best: 30.9005; std: 0.3753)\n",
      "[Generation 134 / 500] score: 30.842 (best: 30.9005; std: 0.0961)\n",
      "[Generation 135 / 500] score: 31.1723 (best: 31.4736; std: 0.2728)\n",
      "[Generation 136 / 500] score: 31.326 (best: 31.4736; std: 0.1285)\n",
      "[Generation 137 / 500] score: 31.4624 (best: 31.6191; std: 0.1625)\n",
      "[Generation 138 / 500] score: 31.7978 (best: 32.0034; std: 0.1935)\n",
      "[Generation 139 / 500] score: 32.1647 (best: 32.3937; std: 0.1998)\n",
      "[Generation 140 / 500] score: 32.2922 (best: 32.3937; std: 0.0879)\n",
      "[Generation 141 / 500] score: 32.6088 (best: 33.1222; std: 0.4465)\n",
      "[Generation 142 / 500] score: 33.0421 (best: 33.1222; std: 0.133)\n",
      "[Generation 143 / 500] score: 33.3484 (best: 33.3698; std: 0.0219)\n",
      "[Generation 144 / 500] score: 33.6798 (best: 33.9125; std: 0.2021)\n",
      "[Generation 145 / 500] score: 33.9405 (best: 34.069; std: 0.117)\n",
      "[Generation 146 / 500] score: 34.3066 (best: 34.7999; std: 0.4273)\n",
      "[Generation 147 / 500] score: 35.1752 (best: 35.2539; std: 0.0852)\n",
      "[Generation 148 / 500] score: 35.5442 (best: 36.061; std: 0.4476)\n",
      "[Generation 149 / 500] score: 35.9094 (best: 36.061; std: 0.1777)\n",
      "[Generation 150 / 500] score: 36.5753 (best: 36.9326; std: 0.3372)\n",
      "[Generation 151 / 500] score: 36.8599 (best: 36.9326; std: 0.095)\n",
      "[Generation 152 / 500] score: 37.2973 (best: 37.5373; std: 0.2339)\n",
      "[Generation 153 / 500] score: 37.7139 (best: 37.9434; std: 0.2081)\n",
      "[Generation 154 / 500] score: 38.3019 (best: 38.7318; std: 0.3991)\n",
      "[Generation 155 / 500] score: 38.9226 (best: 39.746; std: 0.7465)\n",
      "[Generation 156 / 500] score: 39.5973 (best: 39.746; std: 0.1692)\n",
      "[Generation 157 / 500] score: 39.7644 (best: 39.8131; std: 0.0425)\n",
      "[Generation 158 / 500] score: 39.9638 (best: 40.2034; std: 0.2098)\n",
      "[Generation 159 / 500] score: 40.2947 (best: 40.351; std: 0.0798)\n",
      "[Generation 160 / 500] score: 40.4984 (best: 40.8145; std: 0.2739)\n",
      "[Generation 161 / 500] score: 41.1235 (best: 41.4739; std: 0.3317)\n",
      "[Generation 162 / 500] score: 41.5182 (best: 41.723; std: 0.1866)\n",
      "[Generation 163 / 500] score: 42.5866 (best: 42.9353; std: 0.393)\n",
      "[Generation 164 / 500] score: 43.2529 (best: 43.7114; std: 0.4068)\n",
      "[Generation 165 / 500] score: 43.8228 (best: 43.9258; std: 0.1075)\n",
      "[Generation 166 / 500] score: 44.2034 (best: 44.4015; std: 0.2422)\n",
      "[Generation 167 / 500] score: 44.6525 (best: 45.2257; std: 0.4977)\n",
      "[Generation 168 / 500] score: 45.4469 (best: 45.6443; std: 0.2103)\n",
      "[Generation 169 / 500] score: 45.9892 (best: 46.1977; std: 0.2451)\n",
      "[Generation 170 / 500] score: 46.7096 (best: 47.5709; std: 0.7503)\n",
      "[Generation 171 / 500] score: 47.8195 (best: 47.9924; std: 0.2207)\n",
      "[Generation 172 / 500] score: 48.0961 (best: 48.4006; std: 0.2682)\n",
      "[Generation 173 / 500] score: 48.8323 (best: 49.1723; std: 0.3908)\n",
      "[Generation 174 / 500] score: 49.6865 (best: 49.9571; std: 0.2442)\n",
      "[Generation 175 / 500] score: 50.2574 (best: 50.5647; std: 0.289)\n",
      "[Generation 176 / 500] score: 50.981 (best: 51.2695; std: 0.2889)\n",
      "[Generation 177 / 500] score: 51.4267 (best: 52.0286; std: 0.5408)\n",
      "[Generation 178 / 500] score: 52.3981 (best: 52.5362; std: 0.2356)\n",
      "[Generation 179 / 500] score: 52.8015 (best: 53.3363; std: 0.4631)\n",
      "[Generation 180 / 500] score: 53.7151 (best: 54.3283; std: 0.5359)\n",
      "[Generation 181 / 500] score: 54.3938 (best: 55.1351; std: 0.7107)\n",
      "[Generation 182 / 500] score: 55.1873 (best: 55.4486; std: 0.2396)\n",
      "[Generation 183 / 500] score: 56.4336 (best: 56.5968; std: 0.1488)\n",
      "[Generation 184 / 500] score: 57.0493 (best: 57.7916; std: 0.648)\n",
      "[Generation 185 / 500] score: 58.1173 (best: 58.5953; std: 0.423)\n",
      "[Generation 186 / 500] score: 58.7905 (best: 59.0756; std: 0.2525)\n",
      "[Generation 187 / 500] score: 60.9325 (best: 61.8021; std: 0.8115)\n",
      "[Generation 188 / 500] score: 62.0885 (best: 62.3791; std: 0.2885)\n",
      "[Generation 189 / 500] score: 62.9133 (best: 63.7851; std: 0.7598)\n",
      "[Generation 190 / 500] score: 63.3187 (best: 63.7851; std: 0.4061)\n",
      "[Generation 191 / 500] score: 64.8012 (best: 65.5786; std: 0.9203)\n",
      "[Generation 192 / 500] score: 65.8042 (best: 65.9629; std: 0.2007)\n",
      "[Generation 193 / 500] score: 65.9928 (best: 66.0834; std: 0.08)\n",
      "[Generation 194 / 500] score: 66.6889 (best: 67.4027; std: 0.6663)\n",
      "[Generation 195 / 500] score: 68.756 (best: 70.3665; std: 1.4986)\n",
      "[Generation 196 / 500] score: 69.7064 (best: 70.3665; std: 0.572)\n",
      "[Generation 197 / 500] score: 73.2088 (best: 76.434; std: 2.7993)\n",
      "[Generation 198 / 500] score: 77.5732 (best: 79.667; std: 1.8156)\n",
      "[Generation 199 / 500] score: 80.1132 (best: 81.1018; std: 0.8575)\n",
      "[Generation 200 / 500] score: 81.7215 (best: 83.5162; std: 1.5789)\n",
      "[Generation 201 / 500] score: 83.3202 (best: 83.5162; std: 0.3039)\n",
      "[Generation 202 / 500] score: 84.3303 (best: 85.469; std: 1.0161)\n",
      "[Generation 203 / 500] score: 86.8818 (best: 88.5181; std: 1.5368)\n",
      "[Generation 204 / 500] score: 88.4062 (best: 88.7126; std: 0.3751)\n",
      "[Generation 205 / 500] score: 89.3657 (best: 89.6938; std: 0.3609)\n",
      "[Generation 206 / 500] score: 92.2614 (best: 93.7243; std: 1.2669)\n",
      "[Generation 207 / 500] score: 92.9267 (best: 93.7243; std: 1.0798)\n",
      "[Generation 208 / 500] score: 97.251 (best: 98.2091; std: 1.2831)\n",
      "[Generation 209 / 500] score: 98.828 (best: 100.1387; std: 1.1357)\n",
      "[Generation 210 / 500] score: 100.9279 (best: 102.1126; std: 1.0447)\n",
      "[Generation 211 / 500] score: 104.0859 (best: 104.9288; std: 0.8181)\n",
      "[Generation 212 / 500] score: 105.4428 (best: 106.4753; std: 0.8942)\n",
      "[Generation 213 / 500] score: 107.0892 (best: 107.7755; std: 0.6531)\n",
      "[Generation 214 / 500] score: 108.2455 (best: 108.7172; std: 0.4708)\n",
      "[Generation 215 / 500] score: 110.3899 (best: 113.0126; std: 2.2869)\n",
      "[Generation 216 / 500] score: 112.4204 (best: 113.0126; std: 0.9751)\n",
      "[Generation 217 / 500] score: 113.767 (best: 114.3898; std: 0.698)\n",
      "[Generation 218 / 500] score: 114.8582 (best: 116.1443; std: 1.1274)\n",
      "[Generation 219 / 500] score: 118.2586 (best: 120.994; std: 2.4838)\n",
      "[Generation 220 / 500] score: 120.3249 (best: 121.052; std: 1.2094)\n",
      "[Generation 221 / 500] score: 121.7342 (best: 122.5592; std: 0.7637)\n",
      "[Generation 222 / 500] score: 123.6801 (best: 124.2289; std: 0.5807)\n",
      "[Generation 223 / 500] score: 125.5146 (best: 127.2139; std: 1.5349)\n",
      "[Generation 224 / 500] score: 126.6353 (best: 127.5885; std: 1.3397)\n",
      "[Generation 225 / 500] score: 128.7855 (best: 129.8283; std: 1.1278)\n",
      "[Generation 226 / 500] score: 131.2003 (best: 132.3708; std: 1.2832)\n",
      "[Generation 227 / 500] score: 133.3884 (best: 136.2177; std: 2.4822)\n",
      "[Generation 228 / 500] score: 137.1909 (best: 141.7628; std: 4.1712)\n",
      "[Generation 229 / 500] score: 140.1515 (best: 141.7628; std: 2.1703)\n",
      "[Generation 230 / 500] score: 141.6304 (best: 142.1203; std: 0.5677)\n",
      "[Generation 231 / 500] score: 141.6304 (best: 142.1203; std: 0.5677)\n",
      "[Generation 232 / 500] score: 142.1785 (best: 142.6525; std: 0.4477)\n",
      "[Generation 233 / 500] score: 143.078 (best: 144.4611; std: 1.227)\n",
      "[Generation 234 / 500] score: 146.7983 (best: 149.0267; std: 1.9363)\n",
      "[Generation 235 / 500] score: 149.479 (best: 150.2804; std: 0.696)\n",
      "[Generation 236 / 500] score: 150.2712 (best: 151.2605; std: 0.994)\n",
      "[Generation 237 / 500] score: 153.8332 (best: 155.4052; std: 1.3639)\n",
      "[Generation 238 / 500] score: 157.0891 (best: 158.9722; std: 1.7918)\n",
      "[Generation 239 / 500] score: 159.038 (best: 159.5889; std: 0.5211)\n",
      "[Generation 240 / 500] score: 160.469 (best: 161.4688; std: 0.8679)\n",
      "[Generation 241 / 500] score: 165.1582 (best: 166.6268; std: 1.6798)\n",
      "[Generation 242 / 500] score: 166.2937 (best: 166.7333; std: 0.6712)\n",
      "[Generation 243 / 500] score: 167.1357 (best: 167.9237; std: 0.6825)\n",
      "[Generation 244 / 500] score: 169.3796 (best: 170.2581; std: 1.2698)\n",
      "[Generation 245 / 500] score: 171.6202 (best: 172.4223; std: 1.1858)\n",
      "[Generation 246 / 500] score: 172.9532 (best: 173.766; std: 0.7149)\n",
      "[Generation 247 / 500] score: 174.2798 (best: 176.0804; std: 1.6065)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Generation 248 / 500] score: 176.4512 (best: 176.8612; std: 0.3919)\n",
      "[Generation 249 / 500] score: 178.1376 (best: 179.3347; std: 1.2387)\n",
      "[Generation 250 / 500] score: 178.6366 (best: 179.3347; std: 0.6087)\n",
      "[Generation 251 / 500] score: 180.1041 (best: 180.8333; std: 0.6527)\n",
      "[Generation 252 / 500] score: 181.1139 (best: 181.672; std: 0.4833)\n",
      "[Generation 253 / 500] score: 181.8443 (best: 182.3069; std: 0.4049)\n",
      "[Generation 254 / 500] score: 182.96 (best: 183.8782; std: 0.8185)\n",
      "[Generation 255 / 500] score: 183.3929 (best: 183.8782; std: 0.5702)\n",
      "[Generation 256 / 500] score: 184.0975 (best: 184.2283; std: 0.1911)\n",
      "[Generation 257 / 500] score: 185.0326 (best: 185.5232; std: 0.4249)\n",
      "[Generation 258 / 500] score: 185.4698 (best: 185.9347; std: 0.4938)\n",
      "[Generation 259 / 500] score: 186.7455 (best: 186.9978; std: 0.2186)\n",
      "[Generation 260 / 500] score: 187.09 (best: 187.1399; std: 0.08)\n",
      "[Generation 261 / 500] score: 188.5356 (best: 189.1225; std: 0.7929)\n",
      "[Generation 262 / 500] score: 188.9289 (best: 189.1225; std: 0.1687)\n",
      "[Generation 263 / 500] score: 189.0806 (best: 189.2686; std: 0.2121)\n",
      "[Generation 264 / 500] score: 189.3053 (best: 189.3335; std: 0.0333)\n",
      "[Generation 265 / 500] score: 190.1791 (best: 190.5847; std: 0.5397)\n",
      "[Generation 266 / 500] score: 190.7113 (best: 191.1633; std: 0.4039)\n",
      "[Generation 267 / 500] score: 190.9102 (best: 191.1633; std: 0.296)\n",
      "[Generation 268 / 500] score: 191.6878 (best: 192.6928; std: 0.8707)\n",
      "[Generation 269 / 500] score: 192.1309 (best: 192.6928; std: 0.8061)\n",
      "[Generation 270 / 500] score: 192.8719 (best: 193.0525; std: 0.1799)\n",
      "[Generation 271 / 500] score: 194.1777 (best: 195.3555; std: 1.0342)\n",
      "[Generation 272 / 500] score: 195.8521 (best: 196.2058; std: 0.3383)\n",
      "[Generation 273 / 500] score: 195.9143 (best: 196.2058; std: 0.2575)\n",
      "[Generation 274 / 500] score: 196.3828 (best: 196.582; std: 0.1891)\n",
      "[Generation 275 / 500] score: 196.5711 (best: 196.7708; std: 0.2053)\n",
      "[Generation 276 / 500] score: 197.2585 (best: 197.608; std: 0.3028)\n",
      "[Generation 277 / 500] score: 197.7223 (best: 197.8882; std: 0.147)\n",
      "[Generation 278 / 500] score: 197.7888 (best: 197.8882; std: 0.0947)\n",
      "[Generation 279 / 500] score: 198.0164 (best: 198.1651; std: 0.1396)\n",
      "[Generation 280 / 500] score: 198.4156 (best: 198.8564; std: 0.3829)\n",
      "[Generation 281 / 500] score: 199.089 (best: 199.2502; std: 0.2064)\n",
      "[Generation 282 / 500] score: 199.3636 (best: 199.4574; std: 0.0859)\n",
      "[Generation 283 / 500] score: 199.4105 (best: 199.4574; std: 0.0585)\n",
      "[Generation 284 / 500] score: 199.5862 (best: 199.6483; std: 0.0839)\n",
      "[Generation 285 / 500] score: 199.7915 (best: 200.1064; std: 0.2731)\n",
      "[Generation 286 / 500] score: 200.1532 (best: 200.3578; std: 0.1857)\n",
      "[Generation 287 / 500] score: 200.2595 (best: 200.3578; std: 0.0946)\n",
      "[Generation 288 / 500] score: 200.332 (best: 200.3578; std: 0.025)\n",
      "[Generation 289 / 500] score: 200.7457 (best: 201.011; std: 0.2756)\n",
      "[Generation 290 / 500] score: 200.8089 (best: 201.011; std: 0.1842)\n",
      "[Generation 291 / 500] score: 200.8316 (best: 201.011; std: 0.1571)\n",
      "[Generation 292 / 500] score: 200.9454 (best: 201.011; std: 0.0605)\n",
      "[Generation 293 / 500] score: 200.9531 (best: 201.011; std: 0.0509)\n",
      "[Generation 294 / 500] score: 201.0135 (best: 201.0915; std: 0.0767)\n",
      "[Generation 295 / 500] score: 201.0296 (best: 201.0915; std: 0.055)\n",
      "[Generation 296 / 500] score: 201.1588 (best: 201.3739; std: 0.1906)\n",
      "[Generation 297 / 500] score: 201.2001 (best: 201.3739; std: 0.152)\n",
      "[Generation 298 / 500] score: 201.2606 (best: 201.3739; std: 0.1199)\n",
      "[Generation 299 / 500] score: 201.3953 (best: 201.539; std: 0.1343)\n",
      "[Generation 300 / 500] score: 201.409 (best: 201.539; std: 0.1164)\n",
      "[Generation 301 / 500] score: 201.4272 (best: 201.539; std: 0.0968)\n",
      "[Generation 302 / 500] score: 201.494 (best: 201.5498; std: 0.0875)\n",
      "[Generation 303 / 500] score: 201.5705 (best: 201.6228; std: 0.0456)\n",
      "[Generation 304 / 500] score: 201.5705 (best: 201.6228; std: 0.0456)\n",
      "[Generation 305 / 500] score: 201.6032 (best: 201.6371; std: 0.0468)\n",
      "[Generation 306 / 500] score: 201.6556 (best: 201.7069; std: 0.045)\n",
      "[Generation 307 / 500] score: 201.6556 (best: 201.7069; std: 0.045)\n",
      "[Generation 308 / 500] score: 201.6556 (best: 201.7069; std: 0.045)\n",
      "[Generation 309 / 500] score: 201.6556 (best: 201.7069; std: 0.045)\n",
      "[Generation 310 / 500] score: 201.6556 (best: 201.7069; std: 0.045)\n",
      "[Generation 311 / 500] score: 201.6556 (best: 201.7069; std: 0.045)\n",
      "[Generation 312 / 500] score: 201.6556 (best: 201.7069; std: 0.045)\n",
      "[Generation 313 / 500] score: 201.6556 (best: 201.7069; std: 0.045)\n",
      "[Generation 314 / 500] score: 201.6556 (best: 201.7069; std: 0.045)\n",
      "[Generation 315 / 500] score: 201.6556 (best: 201.7069; std: 0.045)\n",
      "[Generation 316 / 500] score: 201.6556 (best: 201.7069; std: 0.045)\n",
      "[Generation 317 / 500] score: 201.661 (best: 201.7069; std: 0.0398)\n",
      "[Generation 318 / 500] score: 201.6648 (best: 201.7069; std: 0.0367)\n",
      "[Generation 319 / 500] score: 201.6648 (best: 201.7069; std: 0.0367)\n",
      "[Generation 320 / 500] score: 201.6883 (best: 201.7095; std: 0.0345)\n",
      "[Generation 321 / 500] score: 201.6883 (best: 201.7095; std: 0.0345)\n",
      "[Generation 322 / 500] score: 201.7069 (best: 201.7095; std: 0.0026)\n",
      "[Generation 323 / 500] score: 201.7198 (best: 201.7431; std: 0.0202)\n",
      "[Generation 324 / 500] score: 201.7535 (best: 201.8078; std: 0.05)\n",
      "[Generation 325 / 500] score: 201.7536 (best: 201.8078; std: 0.0498)\n",
      "[Generation 326 / 500] score: 201.7536 (best: 201.8078; std: 0.0498)\n",
      "[Generation 327 / 500] score: 201.7536 (best: 201.8078; std: 0.0498)\n",
      "[Generation 328 / 500] score: 201.7981 (best: 201.8434; std: 0.0508)\n",
      "[Generation 329 / 500] score: 201.7981 (best: 201.8434; std: 0.0508)\n",
      "[Generation 330 / 500] score: 201.8257 (best: 201.8434; std: 0.0178)\n",
      "[Generation 331 / 500] score: 201.8257 (best: 201.8434; std: 0.0178)\n",
      "[Generation 332 / 500] score: 201.8257 (best: 201.8434; std: 0.0178)\n",
      "[Generation 333 / 500] score: 201.8257 (best: 201.8434; std: 0.0178)\n",
      "[Generation 334 / 500] score: 201.8515 (best: 201.866; std: 0.0126)\n",
      "[Generation 335 / 500] score: 201.8515 (best: 201.866; std: 0.0126)\n",
      "[Generation 336 / 500] score: 201.8515 (best: 201.866; std: 0.0126)\n",
      "[Generation 337 / 500] score: 201.8515 (best: 201.866; std: 0.0126)\n",
      "[Generation 338 / 500] score: 201.8515 (best: 201.866; std: 0.0126)\n",
      "[Generation 339 / 500] score: 201.8515 (best: 201.866; std: 0.0126)\n",
      "[Generation 340 / 500] score: 201.8515 (best: 201.866; std: 0.0126)\n",
      "[Generation 341 / 500] score: 201.8515 (best: 201.866; std: 0.0126)\n",
      "[Generation 342 / 500] score: 201.8515 (best: 201.866; std: 0.0126)\n",
      "[Generation 343 / 500] score: 201.8515 (best: 201.866; std: 0.0126)\n",
      "[Generation 344 / 500] score: 201.8515 (best: 201.866; std: 0.0126)\n",
      "[Generation 345 / 500] score: 201.8751 (best: 201.9144; std: 0.0356)\n",
      "[Generation 346 / 500] score: 201.8751 (best: 201.9144; std: 0.0356)\n",
      "[Generation 347 / 500] score: 201.894 (best: 201.9144; std: 0.0198)\n",
      "[Generation 348 / 500] score: 201.894 (best: 201.9144; std: 0.0198)\n",
      "[Generation 349 / 500] score: 201.894 (best: 201.9144; std: 0.0198)\n",
      "[Generation 350 / 500] score: 201.894 (best: 201.9144; std: 0.0198)\n",
      "[Generation 351 / 500] score: 201.894 (best: 201.9144; std: 0.0198)\n",
      "[Generation 352 / 500] score: 201.8988 (best: 201.9144; std: 0.0136)\n",
      "[Generation 353 / 500] score: 201.8988 (best: 201.9144; std: 0.0136)\n",
      "[Generation 354 / 500] score: 201.8988 (best: 201.9144; std: 0.0136)\n",
      "[Generation 355 / 500] score: 201.8988 (best: 201.9144; std: 0.0136)\n",
      "[Generation 356 / 500] score: 201.8988 (best: 201.9144; std: 0.0136)\n",
      "[Generation 357 / 500] score: 201.8988 (best: 201.9144; std: 0.0136)\n",
      "[Generation 358 / 500] score: 201.8988 (best: 201.9144; std: 0.0136)\n",
      "[Generation 359 / 500] score: 201.9092 (best: 201.9205; std: 0.0147)\n",
      "[Generation 360 / 500] score: 201.9092 (best: 201.9205; std: 0.0147)\n",
      "[Generation 361 / 500] score: 201.9265 (best: 201.9445; std: 0.0159)\n",
      "[Generation 362 / 500] score: 201.9265 (best: 201.9445; std: 0.0159)\n",
      "[Generation 363 / 500] score: 201.9265 (best: 201.9445; std: 0.0159)\n",
      "[Generation 364 / 500] score: 201.927 (best: 201.9445; std: 0.0152)\n",
      "[Generation 365 / 500] score: 201.9355 (best: 201.9445; std: 0.0131)\n",
      "[Generation 366 / 500] score: 201.9355 (best: 201.9445; std: 0.0131)\n",
      "[Generation 367 / 500] score: 201.9613 (best: 201.9979; std: 0.0317)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Generation 368 / 500] score: 201.9613 (best: 201.9979; std: 0.0317)\n",
      "[Generation 369 / 500] score: 201.9613 (best: 201.9979; std: 0.0317)\n",
      "[Generation 370 / 500] score: 201.9613 (best: 201.9979; std: 0.0317)\n",
      "[Generation 371 / 500] score: 201.9613 (best: 201.9979; std: 0.0317)\n",
      "[Generation 372 / 500] score: 201.9613 (best: 201.9979; std: 0.0317)\n",
      "[Generation 373 / 500] score: 201.9613 (best: 201.9979; std: 0.0317)\n",
      "[Generation 374 / 500] score: 201.9613 (best: 201.9979; std: 0.0317)\n",
      "[Generation 375 / 500] score: 201.9613 (best: 201.9979; std: 0.0317)\n",
      "[Generation 376 / 500] score: 201.9613 (best: 201.9979; std: 0.0317)\n",
      "[Generation 377 / 500] score: 201.9613 (best: 201.9979; std: 0.0317)\n",
      "[Generation 378 / 500] score: 201.9613 (best: 201.9979; std: 0.0317)\n",
      "[Generation 379 / 500] score: 201.9613 (best: 201.9979; std: 0.0317)\n",
      "[Generation 380 / 500] score: 201.9613 (best: 201.9979; std: 0.0317)\n",
      "[Generation 381 / 500] score: 201.9613 (best: 201.9979; std: 0.0317)\n",
      "[Generation 382 / 500] score: 201.9613 (best: 201.9979; std: 0.0317)\n",
      "[Generation 383 / 500] score: 201.9613 (best: 201.9979; std: 0.0317)\n",
      "[Generation 384 / 500] score: 201.9613 (best: 201.9979; std: 0.0317)\n",
      "[Generation 385 / 500] score: 201.9629 (best: 201.9979; std: 0.0303)\n",
      "[Generation 386 / 500] score: 201.9754 (best: 201.9979; std: 0.0264)\n",
      "[Generation 387 / 500] score: 201.9754 (best: 201.9979; std: 0.0264)\n",
      "[Generation 388 / 500] score: 201.9754 (best: 201.9979; std: 0.0264)\n",
      "[Generation 389 / 500] score: 202.0312 (best: 202.1138; std: 0.0719)\n",
      "[Generation 390 / 500] score: 202.0312 (best: 202.1138; std: 0.0719)\n",
      "[Generation 391 / 500] score: 202.0312 (best: 202.1138; std: 0.0719)\n",
      "[Generation 392 / 500] score: 202.0312 (best: 202.1138; std: 0.0719)\n",
      "[Generation 393 / 500] score: 202.0312 (best: 202.1138; std: 0.0719)\n",
      "[Generation 394 / 500] score: 202.0628 (best: 202.1138; std: 0.0592)\n",
      "[Generation 395 / 500] score: 202.0662 (best: 202.1138; std: 0.0536)\n",
      "[Generation 396 / 500] score: 202.0662 (best: 202.1138; std: 0.0536)\n",
      "[Generation 397 / 500] score: 202.0662 (best: 202.1138; std: 0.0536)\n",
      "[Generation 398 / 500] score: 202.0662 (best: 202.1138; std: 0.0536)\n",
      "[Generation 399 / 500] score: 202.0662 (best: 202.1138; std: 0.0536)\n",
      "[Generation 400 / 500] score: 202.0827 (best: 202.1138; std: 0.0286)\n",
      "[Generation 401 / 500] score: 202.0827 (best: 202.1138; std: 0.0286)\n",
      "[Generation 402 / 500] score: 202.0827 (best: 202.1138; std: 0.0286)\n",
      "[Generation 403 / 500] score: 202.1248 (best: 202.1839; std: 0.0545)\n",
      "[Generation 404 / 500] score: 202.1248 (best: 202.1839; std: 0.0545)\n",
      "[Generation 405 / 500] score: 202.1723 (best: 202.2193; std: 0.0537)\n",
      "[Generation 406 / 500] score: 202.1725 (best: 202.2193; std: 0.0535)\n",
      "[Generation 407 / 500] score: 202.1725 (best: 202.2193; std: 0.0535)\n",
      "[Generation 408 / 500] score: 202.1725 (best: 202.2193; std: 0.0535)\n",
      "[Generation 409 / 500] score: 202.1725 (best: 202.2193; std: 0.0535)\n",
      "[Generation 410 / 500] score: 202.2058 (best: 202.2193; std: 0.0191)\n",
      "[Generation 411 / 500] score: 202.2058 (best: 202.2193; std: 0.0191)\n",
      "[Generation 412 / 500] score: 202.2058 (best: 202.2193; std: 0.0191)\n",
      "[Generation 413 / 500] score: 202.2058 (best: 202.2193; std: 0.0191)\n",
      "[Generation 414 / 500] score: 202.2058 (best: 202.2193; std: 0.0191)\n",
      "[Generation 415 / 500] score: 202.2058 (best: 202.2193; std: 0.0191)\n",
      "[Generation 416 / 500] score: 202.2058 (best: 202.2193; std: 0.0191)\n",
      "[Generation 417 / 500] score: 202.2058 (best: 202.2193; std: 0.0191)\n",
      "[Generation 418 / 500] score: 202.2058 (best: 202.2193; std: 0.0191)\n",
      "[Generation 419 / 500] score: 202.2058 (best: 202.2193; std: 0.0191)\n",
      "[Generation 420 / 500] score: 202.2264 (best: 202.2457; std: 0.0169)\n",
      "[Generation 421 / 500] score: 202.2264 (best: 202.2457; std: 0.0169)\n",
      "[Generation 422 / 500] score: 202.2264 (best: 202.2457; std: 0.0169)\n",
      "[Generation 423 / 500] score: 202.2264 (best: 202.2457; std: 0.0169)\n",
      "[Generation 424 / 500] score: 202.2264 (best: 202.2457; std: 0.0169)\n",
      "[Generation 425 / 500] score: 202.2264 (best: 202.2457; std: 0.0169)\n",
      "[Generation 426 / 500] score: 202.2264 (best: 202.2457; std: 0.0169)\n",
      "[Generation 427 / 500] score: 202.2264 (best: 202.2457; std: 0.0169)\n",
      "[Generation 428 / 500] score: 202.2264 (best: 202.2457; std: 0.0169)\n",
      "[Generation 429 / 500] score: 202.2264 (best: 202.2457; std: 0.0169)\n",
      "[Generation 430 / 500] score: 202.2264 (best: 202.2457; std: 0.0169)\n",
      "[Generation 431 / 500] score: 202.2264 (best: 202.2457; std: 0.0169)\n",
      "[Generation 432 / 500] score: 202.2264 (best: 202.2457; std: 0.0169)\n",
      "[Generation 433 / 500] score: 202.2264 (best: 202.2457; std: 0.0169)\n",
      "[Generation 434 / 500] score: 202.2276 (best: 202.2457; std: 0.0156)\n",
      "[Generation 435 / 500] score: 202.2276 (best: 202.2457; std: 0.0156)\n",
      "[Generation 436 / 500] score: 202.2279 (best: 202.2457; std: 0.0153)\n",
      "[Generation 437 / 500] score: 202.2279 (best: 202.2457; std: 0.0153)\n",
      "[Generation 438 / 500] score: 202.2279 (best: 202.2457; std: 0.0153)\n",
      "[Generation 439 / 500] score: 202.2279 (best: 202.2457; std: 0.0153)\n",
      "[Generation 440 / 500] score: 202.2279 (best: 202.2457; std: 0.0153)\n",
      "[Generation 441 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 442 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 443 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 444 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 445 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 446 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 447 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 448 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 449 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 450 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 451 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 452 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 453 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 454 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 455 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 456 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 457 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 458 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 459 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 460 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 461 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 462 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 463 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 464 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 465 / 500] score: 202.2287 (best: 202.2457; std: 0.0147)\n",
      "[Generation 466 / 500] score: 202.2333 (best: 202.2457; std: 0.0123)\n",
      "[Generation 467 / 500] score: 202.2333 (best: 202.2457; std: 0.0123)\n",
      "[Generation 468 / 500] score: 202.2333 (best: 202.2457; std: 0.0123)\n",
      "[Generation 469 / 500] score: 202.2333 (best: 202.2457; std: 0.0123)\n",
      "[Generation 470 / 500] score: 202.2333 (best: 202.2457; std: 0.0123)\n",
      "[Generation 471 / 500] score: 202.2333 (best: 202.2457; std: 0.0123)\n",
      "[Generation 472 / 500] score: 202.2333 (best: 202.2457; std: 0.0123)\n",
      "[Generation 473 / 500] score: 202.2333 (best: 202.2457; std: 0.0123)\n",
      "[Generation 474 / 500] score: 202.2333 (best: 202.2457; std: 0.0123)\n",
      "[Generation 475 / 500] score: 202.2482 (best: 202.2659; std: 0.0166)\n",
      "[Generation 476 / 500] score: 202.2482 (best: 202.2659; std: 0.0166)\n",
      "[Generation 477 / 500] score: 202.2482 (best: 202.2659; std: 0.0166)\n",
      "[Generation 478 / 500] score: 202.2482 (best: 202.2659; std: 0.0166)\n",
      "[Generation 479 / 500] score: 202.2482 (best: 202.2659; std: 0.0166)\n",
      "[Generation 480 / 500] score: 202.2482 (best: 202.2659; std: 0.0166)\n",
      "[Generation 481 / 500] score: 202.2482 (best: 202.2659; std: 0.0166)\n",
      "[Generation 482 / 500] score: 202.2482 (best: 202.2659; std: 0.0166)\n",
      "[Generation 483 / 500] score: 202.2482 (best: 202.2659; std: 0.0166)\n",
      "[Generation 484 / 500] score: 202.2482 (best: 202.2659; std: 0.0166)\n",
      "[Generation 485 / 500] score: 202.2482 (best: 202.2659; std: 0.0166)\n",
      "[Generation 486 / 500] score: 202.2482 (best: 202.2659; std: 0.0166)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Generation 487 / 500] score: 202.2482 (best: 202.2659; std: 0.0166)\n",
      "[Generation 488 / 500] score: 202.2482 (best: 202.2659; std: 0.0166)\n",
      "[Generation 489 / 500] score: 202.2482 (best: 202.2659; std: 0.0166)\n",
      "[Generation 490 / 500] score: 202.2482 (best: 202.2659; std: 0.0166)\n",
      "[Generation 491 / 500] score: 202.2616 (best: 202.2733; std: 0.0143)\n",
      "[Generation 492 / 500] score: 202.2616 (best: 202.2733; std: 0.0143)\n",
      "[Generation 493 / 500] score: 202.2616 (best: 202.2733; std: 0.0143)\n",
      "[Generation 494 / 500] score: 202.2616 (best: 202.2733; std: 0.0143)\n",
      "[Generation 495 / 500] score: 202.2616 (best: 202.2733; std: 0.0143)\n",
      "[Generation 496 / 500] score: 202.2616 (best: 202.2733; std: 0.0143)\n",
      "[Generation 497 / 500] score: 202.2616 (best: 202.2733; std: 0.0143)\n",
      "[Generation 498 / 500] score: 202.2616 (best: 202.2733; std: 0.0143)\n",
      "[Generation 499 / 500] score: 202.2616 (best: 202.2733; std: 0.0143)\n",
      "[Generation 500 / 500] score: 202.2816 (best: 202.3057; std: 0.0211)\n",
      "24/24 [==============================] - 0s 696us/step - loss: 4.9418 - mae: 1.3272 - mse: 4.9418\n",
      "Model accuracy: 4.941833019256592\n",
      "\n",
      "Fitting done\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % 100 == 0:\n",
    "            print('')\n",
    "        print('.',end='')\n",
    "\n",
    "# number of \n",
    "EPOCHS = 1000\n",
    "\n",
    "# early stop in case we start overfitting\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# fit the model\n",
    "maxgen = 500 # maxgen = 1000 was the previous one, around 500 we already have ok values\n",
    "# Evolve the model\n",
    "history = model.fit_evolve(normed_train_dataset, train_labels, max_generations=maxgen, population=100, top_k=3, fitness_func=fitness_func)\n",
    "\n",
    "print(f\"Model accuracy: {model.evaluate(normed_test_dataset, test_labels, batch_size=2048)[0]}\")\n",
    "\n",
    "# indicate fitting finished\n",
    "print('\\n' + 'Fitting done')\n",
    "\n",
    "# show history\n",
    "pd.DataFrame(history).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Area  Beta   Tp\n",
      "0    25   2.5  0.1\n",
      "Expected output: \n",
      "11, 4.7, 0.4\n",
      "Recieved:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[20.73087  ,  0.7766812,  0.8241637]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# demonstrate by predicting a set of values\n",
    "d = {'Area':[25],'Beta':[2.5],'Tp':[0.1]}\n",
    "demo_values = pd.DataFrame(d)\n",
    "print(demo_values)\n",
    "# print(type(demo_values))\n",
    "print('Expected output: \\n11, 4.7, 0.4\\nRecieved:\\n')\n",
    "pred = model.predict(norm(demo_values))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:\\Dev\\PyCharmProjects\\ISSlab1\\programs\\saved models\\version-ga-pid\\assets\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# export path\n",
    "# model_dir = tempfile.gettempdir()\n",
    "model_dir = 'D:\\\\Dev\\\\PyCharmProjects\\\\ISSlab1\\\\programs\\\\saved models'\n",
    "# model_dir returns 'C:\\\\Users\\\\Hubert\\\\AppData\\\\Local\\\\Temp'\n",
    "version = \"version-ga-pid\"\n",
    "export_path = os.path.join(model_dir, version)\n",
    "\n",
    "# saving\n",
    "tf.keras.models.save_model(\n",
    "    model,\n",
    "    export_path,\n",
    "    overwrite=True,\n",
    "    include_optimizer=True,\n",
    "    save_format=None,\n",
    "    signatures=None,\n",
    "    options=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
